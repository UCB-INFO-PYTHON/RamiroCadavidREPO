{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W200 Python Fundamentals for Data Science, UC Berkeley MIDS\n",
    "# Final Exam\n",
    "\n",
    "\n",
    "## Instructions\n",
    "The final exam is designed to evaluate your grasp of Python theory as well as Python coding.\n",
    "\n",
    "- This is an individual exam.\n",
    "- You have 24 hours to complete the exam, starting from the point at which you first access it.\n",
    "- You will be graded on the quality of your answers.  Use clear, persuasive arguments based on concepts we covered in class.\n",
    "- While we've left one code/markdown cell for you after each question as a placeholder, some of your answers will require multiple cells to fully respond\n",
    "- Double click the markdown cells where it says YOUR ANSWER HERE to enter your written answers; if you need more cells for your written answers, please make them markdown cells (rather than code cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ramiro Cadavid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: General Questions (21 pts )\n",
    "\n",
    "a) The following method is part of a larger program used by a mobile phone company.  It will work when an object of type MobileDevice or of type ServiceContract is passed in.  This is a demonstration of (select all that apply and state a reason why it applies):\n",
    "\n",
    "    1. Inheritance\n",
    "    2. Polymorphism (X)\n",
    "    3. Duck typing (X)\n",
    "    4. Top-down design \n",
    "    5. Functional programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method:\n",
    "\n",
    "def add_to_cart(item):\n",
    "    cart.append(item)\n",
    "    total += item.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a)\n",
    "    - It is a demonstration of **duck typing** because the method defined accepts any type MobileDevice or ServiceContract whenever these have the method add_to_cart defined. I.e. the method does not check the type of the item that is passes as argument, as long as the item has this method defined.\n",
    "    - It is a demonstration of **polymorphism** because the fact that both MobileDevice and ServiceContract classes have an add_to_cart method, allows for them to have a common interface, even if this method do not have the same functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Suppose you have a long list of digits (0-9) that you want to write to a file.  Would it be more efficient to use ASCII or UTF-8 as an encoding?  How could you create an even smaller binary file to store the information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b) \n",
    "    - It would be more efficient to use ASCII because each character is encoded in 7 bits, while each character in UTF-8 is encoded in at least 1 byte (8 bits). In this case, where all characters are digits from 0 to 9, UTF-8 encoding would require 8 bits to store each digit. Thus, the additional memory allocated to store each digit in UTF-8 would be $n$ more than in ASCII, where $n$ is the number of digits in the list.\n",
    "\n",
    "    - An even smaller binary file could be created by encoding each digit using 4 bits, going from the decimal 0 as the binary 0000 to the decimal 9 as the binary 1001. This method would requitre 3 bits less than ASCII and 4 less than UTF-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) You are part of a team working on a spreadsheet program that is written in Python 3.  The program includes several classes to represent different types of objects that fit into a cell of a spreadsheet.  Give a strong argument for why your team should write an abstract base class to represent such objects and give examples of what should go into such an abstract base class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c)\n",
    "\n",
    "Different data types can share many attributes and methods, independent of the specific feature that each type of data encodes. For example, let's suppose that we have two different data types: profession and ethnicity, then instances of each class would both require to be initialized with a value and would have common validations when an object is initialized, would have common attributes related to the type of values that they can store (e.g string characters, maximum length, etc.) and common methods regarding the transformations that can be done to them (for example, split the value of a multi-select respons into multiple values, remove spaces between words, or capitalize letters). This is even the case if the data types differ significantly. For example, all classes may validate that the value attribute of any object is always an ASCII character, or that its length is not longer than a certain number of characters.\n",
    "\n",
    "Therefore, it does not make sense to recreate these common attributes and methods in each class and it makes more sense to have a base class that groups all of these, and child classes that inherit them; first, because this would be more efficient; second, because it would make the code more readable and less prone to errors; and, third, because it would make the maintainance of the code easier, since a change in one of these common attributes or methods would need to be done only once in the base class, instead of several times in all the the child classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Explain why NumPy is better than lists for \"vectorized\" math operations. Give an example of an operation that is either impossible or painful to implement using traditional Python lists compared to NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d) \n",
    "    - First, because it is more **efficient**. NumPy generalizes scalar operations, applying these to multidimensional arrays at once using a set of highly optimized routines called BLAS (Basic Linear Algebra Subprograms), that make these operations generally faster than using loops with lists.\n",
    "    \n",
    "    - Second, because it is more **concise** (i.e. it generally requires less code to perform the same operation). For example, filtering a array-like object to replace the values that are greater than the mean but less than the mean plus one standard deviation of all values in the array could be done relatively easy with NumPy in one line of code with:\n",
    "    \n",
    "    `a[(a > a.mean()) | (a < a.mean() + a.std())] = X`\n",
    "    \n",
    "    On the contrary (and even more so if the array has more than one dimension), the same operation with a list would require at least one loop for every dimension of the array and several lines of code, making the code not only painful but likely less efficient and prone to errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) We want a list of the numbers that are the square of nonnegative integer less than 10, but whose squares are greater than 10.  The list comprehension below gives an empty list.  Correct it so that we get the desired output: [16, 25, 36, 49, 64, 81]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x**2 for x in range(10) if x ** 2 > 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Explain why the following code prints what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n"
     ]
    }
   ],
   "source": [
    "def f(): pass\n",
    "print(type(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f) This code defines an object of class function named `f` and then returns the type of the object `f` (itself), which is the function just defined (that has type 'function'). This is the reason why the output is \"class 'function'\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Explain why the following code prints something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "def f(): pass\n",
    "print(type(f()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- g) This code defines a function named `f` and calls the function `f()` thus returning the type of its output (not the type of of the function object), which is a `NoneType` since the function doesn't have a `return` statement. This is the reason why the two functions return a different output: the first function prints the type of the object `f`, while the second function returns the type of the object returned by function `f`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Data Integrity (25 pts)\n",
    "\n",
    "a) Why is it important to sanity-check your data before you begin your analysis? What could happen if you don't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a) \n",
    "\n",
    "Any errors and inconsistencies contained in the raw data will be carried over to the analysis. If the data is not checked before the analysis starts two things can happen. In the best case scenario, the errors will be detected later, but this will require a lot of reprocessing, making the process inefficient and prone to additional errors. In the worst case scenario, the errors will pass unnoticed, altering the accuracy of the results. This, in turn can have large consequences depending on the decisions that are made using this data and can reduce the trust that data users have on the results produced by a data science team. \n",
    "\n",
    "Therefore, doing a sanity-check of the data is important because it generally makes the whole data management process much faster by avoiding reprocessing, and will increase the accuracy of the results by assuring that the data used as input complies with minimum standards of quality, adjusted to the importance and consequences of the decisions taken with this data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Explain, in your own words, why real-world data is often messy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b)\n",
    "\n",
    "Real-world data is messy, first, because some of it is hard to measure, either when using human judgement or instruments, since the precision and accuracy required may not be technically feasible. Second, because variables can be poorly defined and poor quality control methods are not implemented (for example, data validation, digital data collection tools that check the data input and prompt users to correct errors, etc.). Third, because of bad design of the structures that store the data, for example, by including data in the name of variables (such as age18-24, age25-32, ...). And, finally, because there may be a poor implementation of quality assurance systems that assess the data captured and make sure that it meets minimum standards of quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) How do you determine which variables in your dataset you should check for issues prior to starting an analysis? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c)\n",
    "\n",
    "To check which variables I should check for issues, I would use the following criteria:\n",
    "\n",
    "- Likelihood of error: based on my knowledge of how this variable is captured, how hard it is to measure correctly?, how accurate are the instruments used to measure it? this will provide a sense of how much the variable can be trusted to be close to the actual value of the dimension that is being measured.\n",
    "- Margin of error: based on my knowledge of how this variable is captured and how likely it is to be inaccurate, how large is the deviation from the actual value? is this deviation large enough to be directly dealt with?\n",
    "- Weight in the analysis: what are the decisions that will be taken with the outputs generated using this variable? what would be the impact of making a wrong call on these decisions? The higher the weight in the analysis, the higher priority a variable will have in the data checking activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) How do you know when you have adequately checked these variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d)\n",
    "\n",
    "The thoroughness with which these variables need to be checked is mainly a function of the third criterion mentioned above: the more important is the decision that needs to be taken and the higher the costs of making a wrong call, the higher the standard is for when it can be determined that these variables have been adequately checked. However, the two other criteria also weight in determining if there has been enough checking of this data: variables that are measured with a good track record of accuracy and precision don't need to be checked as thoroughly and often as the ones that have a poor track record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Is it possible to fully vet your data for errors before you begin your analysis? If not, what should you be looking out for while you complete your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- e)\n",
    "\n",
    "It is not, either because of resources contraints, because of the lack of tools and additional data required to do a more in-depth validation, or because some of the errors can only be detected (or are more easily detected) during the analysis fo the data. For this reason, as the analysis progresses, one should be looking for additional validation of the results against previous results or external sources. One should also try to do sense-checking between different results and contrast them with the available theory, if possible (for example, if I'm analyzing economic variables for a specific country and find that in the same time period both the inflation and unemployment rates are high while the country's growth has slowed, a situation known in economic theory as stagflation, this is a sign that I should check my data again since this is a quite rare event that is unlikely to happen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3:  Elections (24 pts)\n",
    "\n",
    "Consider the following data frame in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>delegates</th>\n",
       "      <th>color</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marco</td>\n",
       "      <td>165</td>\n",
       "      <td>blue</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jeb</td>\n",
       "      <td>0</td>\n",
       "      <td>red</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chris</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>donald</td>\n",
       "      <td>1543</td>\n",
       "      <td>white</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ted</td>\n",
       "      <td>559</td>\n",
       "      <td>blue</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>john</td>\n",
       "      <td>161</td>\n",
       "      <td>red</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  delegates  color state\n",
       "0   marco        165   blue    FL\n",
       "1     jeb          0    red    FL\n",
       "2   chris          0  white    NJ\n",
       "3  donald       1543  white    NY\n",
       "4     ted        559   blue    TX\n",
       "5    john        161    red    OH"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "# creating a data frame from scratch - list of lists\n",
    "\n",
    "data = [ ['marco', 165, 'blue', 'FL'], \n",
    "         ['jeb', 0, 'red', 'FL'], \n",
    "         ['chris', 0, 'white', 'NJ'], \n",
    "         ['donald', 1543, 'white', 'NY'],\n",
    "         ['ted', 559, 'blue', 'TX'],\n",
    "         ['john', 161, 'red', 'OH']\n",
    "       ]\n",
    "\n",
    "# create a data frame with column names - list of lists\n",
    "\n",
    "col_names = ['name', 'delegates', 'color', 'state']\n",
    "df = pandas.DataFrame(data, columns=col_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Using bracket indexing in Pandas, show how many delegates `ted` got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    559\n",
       "Name: delegates, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.name == 'ted']['delegates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Using bracket indexing in Pandas, show how many total delegates were obtained by candidates whose favorite color is blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.color == 'blue']['delegates'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Using groupby and aggregate in Pandas, show how many total delegates were obtained by candidates grouped by favorite color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "color\n",
       "blue      724\n",
       "red       161\n",
       "white    1543\n",
       "Name: delegates, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('color').aggregate(sum)['delegates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Clinical disease data (30 pts)\n",
    "\n",
    "Your boss comes to you Monday morning and says “I figured out our next step; we are going to pivot from an online craft store and become a data center for genetic disease information! I found **ClinVar** which is a repository that contains expert curated data, and it is free for the taking. This is a gold mine! Take a week and tell me what gene and mutation combinations are classified as dangerous.”\n",
    "\n",
    "1)  Look at the sample data set (in the Sample ClinVar data below or in the .txt file) and develop a plan of action to use python to extract and summarize just what your boss wants. **Don’t code**. You can use pseudocode and/or and essay format to generate a plan in 500 words or less. \n",
    "\n",
    "2) Tell us the output that you expect from your planned code\n",
    "\n",
    "**Hints:**  \n",
    "\n",
    "* Look at the sample file carefully. What fields do you want to extract? Are they in the same place every time? What strategy will you use to robustly extract and filter your data of interest? How do you plan to handle missing data?\n",
    "\n",
    "* Filter out junk. Just focus on what your boss asked for (1) gene name (2) mutation reference. (3) Filter your data to include only mutations that are dangerous as you define it. \n",
    "\n",
    "* Pandas and NumPy parsers correctly recognize the end of each line in in the ClinVar file.\n",
    "\n",
    "* The unit of observation of this dataset is one row per mutation.\n",
    "\n",
    "* While you shouldn't code your analysis, creating a few lines of code while you think through the problem may be helpful (so that you can sanity check that your plan works). So you can experiment, we have included the data file below as a Tab Separated Value file \"Genomics_Questions.txt\". Please do not submit any such code. For example, if I wanted to check that I accurately understand the \"split\" function in the context of this data, I could type:\n",
    "\n",
    "```python\n",
    "sample = \"abc;def;asd\"\n",
    "test = sample.split(';')\n",
    "```\n",
    "\n",
    "**This is a planning question we want you to lay out a plan in text not code.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VCF file description (Summarized from version 4.1)\n",
    "\n",
    "\n",
    "* The VCF specification:\n",
    "\n",
    "VCF is a text file format which contains meta-information lines, a header\n",
    "line, and then data lines each containing information about a position in the genome. The format also has the ability to contain genotype information on samples for each position.\n",
    "\n",
    "* Fixed fields:\n",
    "\n",
    "There are 8 fixed fields per record. All data lines are **tab-delimited**. In all cases, missing values are specified with a dot (‘.’). \n",
    "\n",
    "1. CHROM - chromosome number\n",
    "2. POS - position DNA nuceleotide count (bases) along the chromosome\n",
    "3. ID - The unique identifier for each mutation\n",
    "4. REF - reference base(s) alleles\n",
    "5. ALT - alternate base(s) alleles\n",
    "6. QUAL - Phred scaled quality score\n",
    "7. FILTER - filter status (if position has passed all filters)\n",
    "8. INFO - a semicolon-separated series of  keys with values in the format: <key>=<data>, and specified as <key>=<data name>[data value definition].\n",
    "\n",
    "\n",
    "### INFO field specifications\n",
    "\n",
    "```\n",
    "GENEINFO = <Gene symbol>\n",
    "CLNSIG =  <Variant Clinical Significance (Severity)\n",
    "  0 – unknown\t(Uncertain significance)\n",
    "  1 – untested\t(not provided)\n",
    "  2 - non-pathogenic\t(Benign)\n",
    "  3 - probable-non-pathogenic\t(Likely benign)\n",
    "  4 - probable-pathogenic\t(Likely pathogenic)\n",
    "  5 – pathogenic\t(Pathogenic)\n",
    "  6 - drug-response\t(drug response)\n",
    "  7 – histocompatibility\t(histocompatibility)\n",
    "  255 - other\t(other)\n",
    "```\n",
    "\n",
    "### Representative/Sample ClinVar data (vcf file format)\n",
    "\n",
    "```\n",
    "##fileformat=VCFv4.0\t\t\t\t\t\t\t\n",
    "##fileDate=20160705\t\t\t\t\t\t\t\n",
    "##source=ClinVar and dbSNP\t\t\t\t\t\t\t\n",
    "##dbSNP_BUILD_ID=147\t\t\t\t\t\t\t\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\n",
    "1\t949523\trs786201005\tC\tT\t.\t.\tGENEINFO=ISG15;CLNSIG=5\n",
    "1\t949696\trs672601345\tC\tCG\t.\t.\tGENEINFO=ISG15;CLNSIG=5;CLNDBN=Cancer\n",
    "1\t949739\trs672601312\tG\tT\t.\t.\tGENEINFO=ISG15;CLNDBN=Cancer\n",
    "1\t955597\trs115173026\tG\tT\t.\t.\tGENEINFO=AGRN;CLNSIG=2; CLNDBN=Cancer\n",
    "1\t955619\trs201073369\tG\tC\t.\t.\tGENEINFO=AGG;CLNDBN=Heart_dis \n",
    "1\t957640\trs6657048\tC\tT\t.\t.\tGENEINFO=AGG;CLNSIG=3;CLNDBN=Heart_dis \n",
    "1\t976059\trs544749044\tC\tT\t.\t.\tGENEINFO=AGG;CLNSIG=0;CLNDBN=Heart_dis \n",
    "```\n",
    "\n",
    "A second version of this file is provided as a .txt file in case you want to load it into your console to test it out. You can use either file for the data modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Request interpretation\n",
    "\"Take a week and tell me what gene and mutation combinations are classified as dangerous.\" I understand this request as my boss asking me to find all the unique combinations between genes (stored as the GENEINFO=X substring in the INFO column) and mutations (stored in the ID column) that are classified as ('probable-pathogenic' or 'pathogenic', stored as 4 or 5 in the CLNSIG=Y substring of the INFO column) or (cancer or heart disease, stored as Cancer or Heart_dis in the CLNDBN=Z substring of the INFO column).\n",
    "\n",
    "#### Preparation\n",
    "1. Load data from the text file into a data frame as a csv, discarding the first four lines, using the fifth line as the column header and using tab as the separator between columns.\n",
    "1. Remove the '#' character from the name of the first column.\n",
    "1. Transform column names to lowercase.\n",
    "1. Remove all unused columns: 'chrom', 'pos', 'alt', 'qual' and 'filter'.\n",
    "1. Split column 'info' into three columns: 'geneinfo', 'clnsig' and 'clndbn'. The first will contain the substring between 'GENEINFO=' and ';', the second will contain the substring between 'CLNSIG=' and ';', and the last column will contain the substring after 'CLNDBN='. If either of these three substrings is not present, store value as NaN in the corresponding column.\n",
    "1. Remove rows with missing values in both 'clnsig' and 'clndbn'.\n",
    "\n",
    "#### Analysis\n",
    "1. Keep only the columns where: (value in column 'clnsig' is equal to 4 or 5) or (value in column 'clndbn' is equal to Cancer or Heard_dis).\n",
    "1. Create a column named 'gene-mutation' that concatenates the values of the 'geneinfo' and 'id' columns.\n",
    "1. Generate a data frame called dangerous_gene_mutation that contains the unique values in the 'gene-mutation' column, include the geneinfo and id columns.\n",
    "\n",
    "#### Presentation (expected output)\n",
    "1. Create a table with the dangerous_gene_mutation data frame, grouped by gene and mutation, and include the 'danger' classification level columns, i.e. clnsig and clndbn.\n",
    "1. Export this table and share it with my boss. The table should look like this:\n",
    "\n",
    "|Gene  |Mutation|CLNSIG|CLNDBN   |\n",
    "|------|--------|------|---------|\n",
    "|gene_a|mut_1   |4     |NaN      |\n",
    "|      |mut_2   |4     |Heart_dis|\n",
    "|      |mut_3   |1     |Cancer   | \n",
    "|gene_b|mut_4   |5     |Cancer   |\n",
    "|      |mut_5   |4     |NaN      |\n",
    "|gene_c|mut_6   |3     |Cancer   |\n",
    "|      |mut_7   |5     |Hear_dis |\n",
    "|      |mut_8   |4     |NaN      |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
